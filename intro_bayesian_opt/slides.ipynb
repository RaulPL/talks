{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import ax\n",
    "import torch\n",
    "import botorch\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import holoviews as hv\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.acquisition import UpperConfidenceBound, ExpectedImprovement\n",
    "from ax.modelbridge.cross_validation import cross_validate\n",
    "from ax.plot.diagnostic import interact_cross_validation\n",
    "from ax.plot.scatter import plot_fitted\n",
    "from ax.plot.slice import plot_slice\n",
    "from ax.plot.contour import interact_contour, plot_contour\n",
    "from ax.utils.notebook.plotting import render, init_notebook_plotting\n",
    "\n",
    "from IPython.display import Image, IFrame\n",
    "from test_functions import Problem05, Problem07, Problem09, Ursem01, EggCreate\n",
    "\n",
    "hv.extension('bokeh', 'matplotlib')\n",
    "init_notebook_plotting()\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def kernel_function_plot(kernel,\n",
    "                         n_points: int,\n",
    "                         n_samples: int = 3, \n",
    "                         width: int = 400, \n",
    "                         height: int = 400):\n",
    "    \"\"\"Returns plots that describe the behavior of the kernel\"\"\"\n",
    "    # Data\n",
    "    xs = np.linspace(0, 1, n_points)[:,None]\n",
    "    K = kernel(xs).eval()\n",
    "    ys = K[0, :]\n",
    "    samples = pm.MvNormal.dist(mu=np.zeros(K.shape[0]), cov=K).random(size=n_samples).T\n",
    "    # plots\n",
    "    similarity = hv.Curve((xs, ys), vdims=['similarity']).opts(\n",
    "        toolbar=None, ylabel=\"k(0, x)\", height=height, width=width)\n",
    "    similarity = similarity.redim(similarity=hv.Dimension('similarity', range=(0, 1.1)))\n",
    "    points = hv.Scatter((xs, ys), vdims=['similarity']).opts(color='r', marker='*', size=6, alpha=0.5)\n",
    "    cov_matrix_plot = hv.Image(K, kdims=[\"x\", \"y\"], vdims=['k']).opts(\n",
    "        cmap='viridis', colorbar=True, xaxis=None, yaxis=None,\n",
    "        tools=['hover'], ylabel=\"y\", xlabel=\"x\", height=height, width=width)\n",
    "    \n",
    "    curves = {}\n",
    "    for sample in range(n_samples):\n",
    "        c = hv.Curve((xs, samples[:, sample])).opts(height=height, width=width + 250)\n",
    "        sample_points = hv.Scatter((xs, samples[:, sample])).opts(color='k', marker='*', alpha=0.3, size=3)\n",
    "        curves[sample] = c * sample_points\n",
    "\n",
    "    nd_overlay = hv.NdOverlay(curves, kdims=['samples']).opts(toolbar=None)\n",
    "    nd_overlay = nd_overlay.opts(\n",
    "        {'Curve': {'color': hv.Cycle('Category10')}}\n",
    "    )\n",
    "    plot = (\n",
    "        (similarity * points) + cov_matrix_plot + nd_overlay\n",
    "    ).opts(hv.opts.Layout(shared_axes=False, shared_datasource=False))\n",
    "    return plot\n",
    "\n",
    "\n",
    "def get_sobel_seq(n: int, lower_bound: float, upper_bound: float):\n",
    "    \"\"\"Generates a Sobol sequence o n points\"\"\"\n",
    "    if lower_bound > upper_bound:\n",
    "        raise ValueError('upper_bound can not be smaller than lower_bound')\n",
    "    sobol = ax.Models.SOBOL(\n",
    "        search_space=ax.SearchSpace(\n",
    "            parameters=[\n",
    "                ax.RangeParameter(\n",
    "                    name=\"x\", \n",
    "                    lower=lower_bound, \n",
    "                    upper=upper_bound,\n",
    "                    parameter_type=ax.ParameterType.FLOAT, \n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    sobol_seq = [arm.parameters['x'] for arm in sobol.gen(N_INIT).arms]\n",
    "    return sobol_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Bayesian optimization\n",
    "\n",
    "__Slides__:\n",
    "\n",
    "## Ra√∫l Peralta Lozada\n",
    "\n",
    "* Blog: https://raulpl.github.io/\n",
    "* Twitter: @tnak25\n",
    "* email: raulpl25@gmail.com\n",
    "* OPI Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./figures/OPI_logo.jpeg\" alt=\"opi_logo\" title=\"opi_logo\" width=\"600\" height=\"50\" />\n",
    "<img src=\"./figures/OPI_team.jpg\" alt=\"opi\" title=\"opi\" width=\"1000\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objectives:\n",
    "\n",
    "1. Understand the general concepts behind Bayesian optimization\n",
    "2. Use Python to solve this kind of optimization problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization \n",
    "Find the global maximizer (or minimizer) of an objective function $f$ in some design space $\\mathcal{X}$\n",
    "\n",
    "$$x^{*} = argmax_{x \\in \\mathcal{X}} f(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "hv.Store.current_backend = 'matplotlib'\n",
    "xs = np.linspace(-3, 3, 100)\n",
    "ys = - np.power(xs, 2)\n",
    "parabola = (\n",
    "    hv.Curve((xs, ys)).opts(title=r\"$ y=-x^2 $\", linewidth=3) * \n",
    "    hv.VLine(0, label='h').opts(color='red') *\n",
    "    hv.Labels(([1.5], [0], r'$\\frac{dy}{dx}=-2x=0$')).opts(size=13)\n",
    ").opts(fig_size=200, fontsize={'labels': 14})\n",
    "parabola.redim(y=hv.Dimension('y', range=(-8, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problems in real world applications:\n",
    "\n",
    "* Black box functions\n",
    "* Noisy evaluations\n",
    "* Restricted evaluation budget\n",
    "* Unknown gradient information\n",
    "\n",
    "### Examples:\n",
    "\n",
    "* A/B testing\n",
    "* Hyperparameter tunning\n",
    "* Resource extraction\n",
    "* Robotics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "problem_unknown = Problem05()\n",
    "points_unknown = [(0.5, problem_unknown(0.5)),]\n",
    "\n",
    "def interactive_optimization(x, y):\n",
    "    if x:\n",
    "        y = problem_unknown(x) + float(np.random.normal(scale=0.2, size=1))\n",
    "        points_unknown.append((x, y))\n",
    "    points_plot = hv.Points((points_unknown)).opts(color='k', marker='*', size=12, width=900, height=600)\n",
    "    points_plot = points_plot.redim(\n",
    "        x=hv.Dimension('x', range=(problem_unknown.bounds[0], problem_unknown.bounds[1])),\n",
    "        y=hv.Dimension('y', range=(-2.2, 2.2))\n",
    "    )\n",
    "    plot = points_plot\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# hv.Store.current_backend = 'bokeh'\n",
    "# tap = hv.streams.Tap(transient=False)\n",
    "# dmap = hv.DynamicMap(interactive_optimization, streams=[tap]).opts(tools=['hover', 'tap']).opts(framewise=True)\n",
    "# dmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "xs = np.linspace(*problem_unknown.bounds, num=200)\n",
    "ys = np.array([problem_unknown(x) for x in xs])\n",
    "plot = (\n",
    "    hv.Curve((xs, ys)) * \n",
    "    hv.Scatter(points_unknown).opts(marker='*', size=10, color='k')\n",
    ").opts(width=900, height=600)\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exploration-exploitation dilemma\n",
    "\n",
    "\n",
    "__Exploration__: collect information about the domain\n",
    "\n",
    "__Exploitation__: make the best decision given the current information\n",
    "\n",
    "<sup>http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/XX.pdf<sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian optimization\n",
    "\n",
    "Bayesian optimization is a sequential design strategy for global optimization of black-box functions.\n",
    "\n",
    "### Key components:\n",
    "* *__Statistical model / surrogate model__*: captures the current belief about of the latent function $f$. \n",
    "* *__Acquisition function__*: evaluates the utility of candidate points for the next evaluation of $f$. This function determines the exploration strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algorithm\n",
    "* Unknown noisy objective function $f$\n",
    "* Input domain $\\mathcal{X}$\n",
    "* Fixed query budget $T$\n",
    "* Initial sample (optional) $D$\n",
    "\n",
    "for n = 1, 2, ... $T$, do\n",
    "1. Select new $x_{n + 1}$ by optimizing __acquisition function__ $\\alpha$  \n",
    "    $x_{n + 1} = argmax_{x \\in \\mathcal{X}} \\alpha(x; D_n)$\n",
    "    \n",
    "2. Query objective function $f$ to obtain $y_{n + 1}$\n",
    "3. Augment data $D_{n + 1} = \\{ D_{n}, (x_{n + 1}, y_{n + 1}) \\}$\n",
    "4. Update __statistical model__\n",
    "\n",
    "<sub>[Taking the Human Out of the Loop: A Review of Bayesian Optimization](https://ieeexplore.ieee.org/document/7352306)<sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistical model\n",
    "\n",
    "We need a model that can:\n",
    "* __Learn__ from previous observations\n",
    "* __Predict__ the value of new points\n",
    "* Quantify the level of __uncertainty__ of those predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gaussian process (GP)\n",
    "\n",
    "A GP is a collection of random variables, any finite number of which have a joint Gaussian distribution. This model is used to describe a distribution over functions. GPs are completely specified by:\n",
    "1. A mean function $\\mu(x)$ that is the average of all functions.\n",
    "2. A covariance function or kernel $k(x, x')$ that describes our assumptions over the structure of the possible set of functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "IFrame('https://distill.pub/2019/visual-exploration-gaussian-processes/', width=1400, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Multivariate Gaussian distribution\n",
    "mean = [0, 0]\n",
    "cov = [\n",
    "    (1, -0.9),\n",
    "    (-0.9, 1)\n",
    "]\n",
    "mv_gaussian_sample = np.random.multivariate_normal(mean, cov, 1_000)\n",
    "hv.Points(mv_gaussian_sample, kdims=['x1', 'x2']).opts(alpha=0.4, size=5, width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Squared Exponential / Exponentiated Quadratic:  \n",
    "* $k(x, x') = \\mathrm{exp}\\left[ -\\frac{(x - x')^2}{2 \\ell^2} \\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Example taken from:\n",
    "# https://colab.research.google.com/github/fonnesbeck/Bios8366/blob/master/notebooks/Section5_1-Gaussian-Processes.ipynb\n",
    "hv.Store.current_backend = 'bokeh'\n",
    "ls = 1\n",
    "kernel = pm.gp.cov.ExpQuad(1, ls=ls)\n",
    "kernel_function_plot(kernel, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matern 3/2\n",
    "* $k(x, x') = \\left(1 + \\frac{\\sqrt{3(x - x')^2}}{\\ell}\\right) \\mathrm{exp}\\left[ - \\frac{\\sqrt{3(x - x')^2}}{\\ell} \\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ls = 0.1\n",
    "kernel = pm.gp.cov.Matern32(1, ls=ls)\n",
    "kernel_function_plot(kernel, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Periodic\n",
    " * $k(x, x') = \\exp\\left( -\\frac{2 \\sin^{2}(\\pi |x - x'|\\frac{1}{T})}{\\ell^2}     \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ls = 0.6\n",
    "period = 0.3\n",
    "kernel = pm.gp.cov.Periodic(1, ls=ls, period=period)\n",
    "kernel_function_plot(kernel, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Kernel composition\n",
    " \n",
    "Locally periodic = __Periodic__ $\\times$ __Squared Exponential__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ls1 = 0.6\n",
    "period = 0.3\n",
    "ls2 = 0.3\n",
    "kernel = pm.gp.cov.Periodic(1, ls=ls1, period=period) * pm.gp.cov.ExpQuad(1, ls=ls2)\n",
    "kernel_function_plot(kernel, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def mean_cov_plot(x_train, y_train, x_test, trace, GP):\n",
    "    mu, cov = GP.predict(x_test, point=trace[-1], diag=True)\n",
    "    sd = np.sqrt(cov)\n",
    "    sd_region = hv.Area((x_test, mu - 2 * sd, mu + 2 * sd), \n",
    "                        vdims=['lower', 'upper'], label='¬±2sd(ùë•)').opts(alpha=0.2)\n",
    "    mean_curve = hv.Curve((x_test, mu), label='ùúá(ùë•)').opts(color='red')\n",
    "    train_points = hv.Scatter(\n",
    "        (x_train, y_train), \n",
    "        label='observed').opts(color='k', marker='*', size=8, legend_position='bottom_left')\n",
    "    plot = (mean_curve * sd_region * train_points).opts(width=600, height=380, toolbar=None)\n",
    "    return plot\n",
    "\n",
    "\n",
    "def sample_model(model: pm.Model, trace, GP, test_x):\n",
    "    with model:\n",
    "        f_pred = GP.conditional('f_pred', test_x)\n",
    "        pred_samples = pm.sample_posterior_predictive(trace, vars=[f_pred], samples=100)\n",
    "    return pred_samples['f_pred']\n",
    "\n",
    "\n",
    "def samples_plot(x_train, y_train, x_test, samples):\n",
    "    train_points = hv.Scatter((x_train, y_train)).opts(color='k', marker='*', size=8)\n",
    "    curves = hv.Path((x_test, samples.T)).opts(alpha=0.3, color='orange')\n",
    "    plot = (curves * train_points).opts(width=600, toolbar=None)\n",
    "    return plot\n",
    "\n",
    "\n",
    "def function_plot(xs, ys, train_x, train_y):\n",
    "    curve = hv.Curve((xs, ys)).opts(width=600, height=380, toolbar=None)\n",
    "    train_points = hv.Scatter(\n",
    "        (train_x, train_y), label='observed'\n",
    "    ).opts(color='k', marker='*', size=8, toolbar=None, legend_position='bottom_left')\n",
    "    plot = curve * train_points\n",
    "    return plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "N_POINTS: int = 150\n",
    "N_INIT: int = 16\n",
    "problem = Problem09()\n",
    "xs = np.linspace(*problem.bounds, num=N_POINTS)\n",
    "ys = np.array([problem(x) for x in xs])\n",
    "\n",
    "minmax_scaler = MinMaxScaler().fit(xs[:, np.newaxis])\n",
    "standar_scaler = StandardScaler().fit(ys[:, np.newaxis])\n",
    "\n",
    "xs_normalized = minmax_scaler.transform(xs[:, np.newaxis])\n",
    "ys_standardized = standar_scaler.transform(ys[:, np.newaxis])\n",
    "\n",
    "sobol_seq = get_sobel_seq(N_INIT, problem.bounds[0], problem.bounds[1])\n",
    "train_x = np.array(sobol_seq)\n",
    "\n",
    "train_y = np.array([problem(x) for x in train_x])\n",
    "train_y = train_y + np.random.normal(scale=0.2, size=N_INIT)\n",
    "train_x = minmax_scaler.transform(train_x[:, None])\n",
    "train_y = standar_scaler.transform(train_y[:, None])\n",
    "\n",
    "train_iterations = {}\n",
    "for i in range(N_INIT):\n",
    "    train_x_i = train_x[0:i + 1]\n",
    "    train_y_i = train_y[0:i + 1]\n",
    "    train_iterations[i] = (train_x_i, train_y_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def train_pymc3_gp(train_x, train_y):\n",
    "    # Model definition\n",
    "    with pm.Model() as model:\n",
    "        ls = pm.Gamma('ls', alpha=2, beta=9)\n",
    "        e = pm.Gamma('e', alpha=2, beta=9)\n",
    "        cov = pm.gp.cov.ExpQuad(1, ls=ls)\n",
    "        GP = pm.gp.Marginal(cov_func=cov)\n",
    "        y_pred = GP.marginal_likelihood('y_pred', X=train_x, y=train_y, noise=e)\n",
    "        prior = pm.sample_prior_predictive()\n",
    "        trace = pm.sample(1_200, tune=1_000)\n",
    "        posterior_predictive = pm.sample_posterior_predictive(trace)\n",
    "    return model, trace, GP\n",
    "\n",
    "\n",
    "train_plots = {}\n",
    "for i, (train_x, train_y) in train_iterations.items():\n",
    "    print(f'Iteration: {i}')\n",
    "    model, trace, GP = train_pymc3_gp(train_x, train_y.flatten())\n",
    "    plot = (\n",
    "        function_plot(xs_normalized, ys_standardized, train_x, train_y) + \n",
    "        mean_cov_plot(train_x, train_y, xs_normalized, trace, GP)\n",
    "    ).cols(1)\n",
    "    train_plots[i] = plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model, trace, GP = train_pymc3_gp(train_x, train_y.flatten())\n",
    "f_samples = sample_model(model, trace, GP, xs_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GP training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "hmap = hv.HoloMap(train_plots, kdims=['i']).opts(framewise=True).collate()\n",
    "hmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Posterior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "(samples_plot(train_x, train_y, xs_normalized, f_samples) * function_plot(xs_normalized, ys_standardized, train_x, train_y)).opts(width=800, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Acquisition functions\n",
    "\n",
    "Acquisition functions are heuristics employed to evaluate the usefulness of one of more design points for achieving the objective of maximizing the underlying black box function<sup>1</sup> . The predictions and the uncertainty estimates are combined to derive the explorarion strategies. Common acquisition functions are:\n",
    "\n",
    "* Upper confidence bound\n",
    "* Thompson sampling\n",
    "* Probability of improvement\n",
    "* Expected improvement\n",
    "* Predictive entropy search\n",
    "\n",
    "<sup>1</sup>[https://botorch.org/docs/acquisition](https://botorch.org/docs/acquisition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Image('./figures/overview_bayesopt.svg.png', width=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Upper Confidence Bound (UCB)\n",
    "\n",
    "*Optimism in the face of uncertainty*\n",
    "\n",
    "UCB is a function that trades off between greedy maximization and uncertainty reduction by taking into account the current value of the mean function and the value of the (weighted) standard deviation for each point. $\\beta$ controls the explore-exploit trade-off.\n",
    "\n",
    "$$\\alpha_{UCB}(x; D_n) = \\mu_{n}(x) + \\beta_{n} \\sqrt{\\sigma_{n}(x)}$$ \n",
    "$$\\beta_{n} \\geqslant 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def predict_mean_and_var(model, test_x):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        f_preds = model(test_x)\n",
    "        f_mean = f_preds.mean\n",
    "        f_var = f_preds.variance\n",
    "    return f_mean.numpy(), f_var.numpy()\n",
    "\n",
    "\n",
    "def mean_sd_plot(mean_pred, var_pred, test_x):\n",
    "    sd = np.sqrt(var_pred)\n",
    "    sd_region = hv.Area((test_x, mean_pred - 2 * sd, mean_pred + 2 * sd),\n",
    "                        vdims=['lower', 'upper'], label='¬±2sd(ùë•)').opts(alpha=0.2, toolbar=None)\n",
    "    mean_curve = hv.Curve((test_x, mean_pred), label='ùúá(ùë•)').opts(color='red', width=600, height=380)\n",
    "    plot = (mean_curve * sd_region)\n",
    "    return plot\n",
    "    \n",
    "\n",
    "def acquisition_function_eval(acq_f, test_x):\n",
    "    acq_f.eval()\n",
    "    with torch.no_grad():\n",
    "        evaluations = acq_f(test_x.unsqueeze(2))\n",
    "    return evaluations.numpy()\n",
    "\n",
    "\n",
    "def acquisition_function_view(test_x, acq_func_evals, width=600, height=380):\n",
    "    best_point = float(test_x[acq_func_evals.argmax()])\n",
    "    acq_min, acq_max = float(acq_func_evals.min()), float(acq_func_evals.max())\n",
    "    curve = hv.Curve(\n",
    "        (test_x, acq_func_evals), vdims=['alpha']\n",
    "    ).opts(width=width, height=height, toolbar=None)\n",
    "    curve = curve.redim(alpha=hv.Dimension('alpha', range=(acq_min - 0.1, acq_max + 0.1)))\n",
    "    vline = hv.VLine(best_point).opts(color='red', line_width=4)\n",
    "    return curve * vline\n",
    "\n",
    "\n",
    "def training_data_view(train_x, train_y):\n",
    "    \"Scatter plot of the training data\"\n",
    "    train_points_scatter = hv.Scatter(\n",
    "        (train_x, train_y), label='observed'\n",
    "    ).opts(color='k', marker='*', size=8, legend_position='bottom_left')\n",
    "    return train_points_scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Setup Upper Confidence Bound\n",
    "\n",
    "N_INIT = 5\n",
    "N_POINTS = 500\n",
    "N_TRIALS: int = 21\n",
    "NOISE_SCALE = 0.2\n",
    "BETA = 20\n",
    "problem = Problem09()\n",
    "\n",
    "\n",
    "xs = np.linspace(*problem.bounds, num=N_POINTS)\n",
    "ys = np.array([problem(x) for x in xs])\n",
    "\n",
    "minmax_scaler = MinMaxScaler().fit(xs[:, np.newaxis])\n",
    "standar_scaler = StandardScaler().fit(ys[:, np.newaxis])\n",
    "\n",
    "xs_normalized = minmax_scaler.transform(xs[:, np.newaxis])\n",
    "ys_standardized = standar_scaler.transform(ys[:, np.newaxis])\n",
    "\n",
    "sobol_seq = get_sobel_seq(N_INIT, problem.bounds[0], problem.bounds[1])\n",
    "\n",
    "xs_init = np.array(sobol_seq)\n",
    "ys_init = np.array([problem(x) for x in xs_init])\n",
    "ys_init = ys_init + np.random.normal(scale=NOISE_SCALE, size=N_INIT)\n",
    "xs_init = minmax_scaler.transform(xs_init[:, None])\n",
    "ys_init = standar_scaler.transform(ys_init[:, None])\n",
    "train_x = torch.FloatTensor(xs_init)\n",
    "train_y = torch.FloatTensor(ys_init)\n",
    "\n",
    "# Surrogate model (GP)\n",
    "gp = SingleTaskGP(train_x, train_y)\n",
    "\n",
    "# marginal log likelihood (loss function)\n",
    "mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "mll = fit_gpytorch_model(mll, max_retries=100)\n",
    "\n",
    "test_points = torch.FloatTensor(xs_normalized)\n",
    "bounds = torch.stack([torch.zeros(1), torch.ones(1)])\n",
    "\n",
    "plots = {}\n",
    "for i in range(N_TRIALS):\n",
    "    gp = SingleTaskGP(train_x, train_y)\n",
    "    mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "    mll = fit_gpytorch_model(mll, max_retries=50)\n",
    "    UCB = UpperConfidenceBound(gp, beta=BETA, maximize=True)\n",
    "    new_x, _ = optimize_acqf(\n",
    "        UCB, bounds=bounds, q=1, num_restarts=100, raw_samples=100,\n",
    "    )\n",
    "    new_x_unscaled = minmax_scaler.inverse_transform(new_x.numpy())\n",
    "    new_y = np.array(problem(float(new_x_unscaled[0][0])))\n",
    "    # add noise\n",
    "    new_y = new_y + np.random.normal(scale=NOISE_SCALE, size=1)\n",
    "    new_y = standar_scaler.transform(new_y.reshape((1,1)))\n",
    "    new_y = torch.FloatTensor(new_y)\n",
    "    # Save evaluations\n",
    "    f_mean, f_var = predict_mean_and_var(gp, test_points)\n",
    "    ucb_evals = acquisition_function_eval(UCB, test_points)\n",
    "    layout = (\n",
    "        mean_sd_plot(f_mean, f_var, test_points.numpy()) * \n",
    "        training_data_view(train_x.numpy(), train_y.numpy()) \n",
    "    )\n",
    "    layout = (layout + acquisition_function_view(test_points.numpy(), ucb_evals)).cols(1)\n",
    "    plots[i + N_INIT] = layout\n",
    "    # Update training \n",
    "    train_x = torch.cat([train_x, new_x])\n",
    "    train_y = torch.cat([train_y, new_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Upper Confidence Bound\n",
    "hmap = hv.HoloMap(plots, kdims=['n_points']).opts(framewise=True).collate()\n",
    "hmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Expected improvement (EI)\n",
    "\n",
    "EI is one of the most commonly used acquisition due to its practical performance and its analytical solution under the GP model.\n",
    "\n",
    "\n",
    "$$\\text{EI}(x) = \\mathbb{E}\\bigl[\\max(f(x) - f^*, 0)\\bigr] = \\int \\max(f(x) - f^*, 0) p(f(x)|x, D) df$$\n",
    "\n",
    "<sup>[A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning](https://arxiv.org/abs/1012.2599)<sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Image('./figures/probanility_of_improvement.png', width=800, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Setup Expected improvement\n",
    "\n",
    "N_INIT = 5\n",
    "N_POINTS = 500\n",
    "NOISE_SCALE = 0.2\n",
    "problem = Problem09()\n",
    "\n",
    "xs = np.linspace(*problem.bounds, num=N_POINTS)\n",
    "ys = np.array([problem(x) for x in xs])\n",
    "\n",
    "minmax_scaler = MinMaxScaler().fit(xs[:, np.newaxis])\n",
    "standar_scaler = StandardScaler().fit(ys[:, np.newaxis])\n",
    "\n",
    "xs_normalized = minmax_scaler.transform(xs[:, np.newaxis])\n",
    "ys_standardized = standar_scaler.transform(ys[:, np.newaxis])\n",
    "\n",
    "sobol_seq = get_sobel_seq(N_INIT, problem.bounds[0], problem.bounds[1])\n",
    "\n",
    "xs_init = np.array(sobol_seq)\n",
    "ys_init = np.array([problem(x) for x in xs_init])\n",
    "ys_init = ys_init + np.random.normal(scale=NOISE_SCALE, size=N_INIT)\n",
    "xs_init = minmax_scaler.transform(xs_init[:, None])\n",
    "ys_init = standar_scaler.transform(ys_init[:, None])\n",
    "train_x = torch.FloatTensor(xs_init)\n",
    "train_y = torch.FloatTensor(ys_init)\n",
    "\n",
    "# Surrogate model (GP)\n",
    "gp = SingleTaskGP(train_x, train_y)\n",
    "\n",
    "# marginal log likelihood (loss function)\n",
    "mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "mll = fit_gpytorch_model(mll, max_retries=100)\n",
    "\n",
    "N_TRIALS: int = 21\n",
    "test_points = torch.FloatTensor(xs_normalized)\n",
    "bounds = torch.stack([torch.zeros(1), torch.ones(1)])\n",
    "\n",
    "plots = {}\n",
    "\n",
    "for i in range(N_TRIALS):\n",
    "    gp = SingleTaskGP(train_x, train_y)\n",
    "    mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "    mll = fit_gpytorch_model(mll, max_retries=50)\n",
    "    # Take the current best value\n",
    "    best_f = train_y.max()\n",
    "    EI = ExpectedImprovement(gp, best_f=best_f, maximize=True)\n",
    "    new_x, _ = optimize_acqf(\n",
    "        EI, bounds=bounds, q=1, num_restarts=100, raw_samples=100\n",
    "    )\n",
    "    new_x_unscaled = minmax_scaler.inverse_transform(new_x.numpy())\n",
    "    new_y = np.array(problem(float(new_x_unscaled[0][0])))\n",
    "    # add noise\n",
    "    new_y = new_y + np.random.normal(scale=NOISE_SCALE, size=1)\n",
    "    new_y = standar_scaler.transform(new_y.reshape((1,1)))\n",
    "    new_y = torch.FloatTensor(new_y)\n",
    "    # Save evaluations\n",
    "    f_mean, f_var = predict_mean_and_var(gp, test_points)\n",
    "    ei_evals = acquisition_function_eval(EI, test_points)\n",
    "    layout = (\n",
    "        mean_sd_plot(f_mean, f_var, test_points.numpy()) * \n",
    "        training_data_view(train_x.numpy(), train_y.numpy()) \n",
    "    )\n",
    "    layout = (layout + acquisition_function_view(test_points.numpy(), ei_evals)).cols(1)\n",
    "    plots[i + N_INIT] = layout\n",
    "    # Update training \n",
    "    train_x = torch.cat([train_x, new_x])\n",
    "    train_y = torch.cat([train_y, new_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Expected Improvement\n",
    "hmap = hv.HoloMap(plots, kdims=['n_points']).opts(framewise=True).collate()\n",
    "hmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./figures/ax_botorch.jpg\" alt=\"botorch\" title=\"Botorch\" width=\"650\" height=\"500\" />\n",
    "\n",
    "*__Botorch__*: is a library for Bayesian optimization research built on top of __PyTorch__ and __GPyTorch__.\n",
    "\n",
    "*__Ax__*: is an adaptive experimentation platform. It provides easy-to-use APIs to interface with BoTorch, along with the management necessary for production-ready services and reproducible research. This allows developers to focus on the applied problems, such as exploring configurations and understanding trade-offs between objectives.\n",
    "\n",
    "<sub>[Open-sourcing Ax and BoTorch: New AI tools for adaptive experimentation](https://ai.facebook.com/blog/open-sourcing-ax-and-botorch-new-ai-tools-for-adaptive-experimentation/)<sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Image('./figures/botorch_ax_diagram.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "POINTS = 100\n",
    "problem = EggCreate()\n",
    "points = np.linspace(problem.bounds[0][0], problem.bounds[1][0], POINTS)\n",
    "zs = np.empty((POINTS, POINTS))\n",
    "for x in range(POINTS):\n",
    "    for y in (range(POINTS)):\n",
    "        zs[y, x] = problem(points[x], points[y])\n",
    "fig = go.Figure(data=[go.Surface(z=zs, colorscale='Viridis')])\n",
    "fig.update_layout(title=f'{problem.__class__.__name__}', autosize=False, width=1_000, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "N_INIT: int = 5\n",
    "N_TRIALS: int = 50\n",
    "NOISE_SCALE = 0.1\n",
    "objective_name = f'{problem.__class__.__name__}'\n",
    "\n",
    "# Search space\n",
    "search_space = ax.SearchSpace(\n",
    "    parameters=[\n",
    "        ax.RangeParameter(\n",
    "            name='x', parameter_type=ax.ParameterType.FLOAT, \n",
    "            lower=problem.bounds[0][0], upper=problem.bounds[1][0]\n",
    "        ),\n",
    "        ax.RangeParameter(\n",
    "            name='y', parameter_type=ax.ParameterType.FLOAT,\n",
    "            lower=problem.bounds[0][1], upper=problem.bounds[1][1]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Experiment object\n",
    "experiment = ax.SimpleExperiment(\n",
    "    name=f'test_{objective_name}',\n",
    "    search_space=search_space,\n",
    "    evaluation_function=lambda p: problem(p[\"x\"], p[\"y\"]) + float(np.random.normal(scale=NOISE_SCALE, size=1)),\n",
    "    objective_name=objective_name,\n",
    "    minimize=False,\n",
    ")\n",
    "\n",
    "# Initial samples\n",
    "sobol = ax.Models.SOBOL(experiment.search_space)\n",
    "for i in range(N_INIT):\n",
    "    experiment.new_trial(generator_run=sobol.gen(1))\n",
    "\n",
    "# Bayesian optmization loop\n",
    "best_arm = None\n",
    "for i in range(N_TRIALS):\n",
    "    gpei = ax.Models.GPEI(experiment=experiment, data=experiment.eval())\n",
    "    generator_run = gpei.gen(1)\n",
    "    best_arm, _ = generator_run.best_arm_predictions\n",
    "    experiment.new_trial(generator_run=generator_run)\n",
    "    \n",
    "model = ax.Models.GPEI(experiment=experiment, data=experiment.eval())\n",
    "best_parameters = best_arm.parameters\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "render(plot_contour(model=model, param_x=\"x\", param_y=\"y\", metric_name=objective_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cv_results = cross_validate(model)\n",
    "render(interact_cross_validation(cv_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "render(plot_slice(model, \"x\", objective_name, slice_values=best_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "render(plot_slice(model, \"y\", objective_name, slice_values=best_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "* Explore-exploit trade-off.\n",
    "* Bayesian optimization is a technique that can help us find the best parameters of a function in a systematic manner.\n",
    "* Careful choice of statistical model is more important than the choice of an acquisition function.\n",
    "* Python has libraries that can help you solve this kind of problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resources\n",
    "\n",
    "* [Taking the Human Out of the Loop: A Review of Bayesian Optimization](https://ieeexplore.ieee.org/document/7352306)\n",
    "* [A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning](https://arxiv.org/abs/1012.2599)\n",
    "* [Bayesian optimization UAI 2018](https://www.youtube.com/watch?v=C5nqEHpdyoE)\n",
    "* [Open-sourcing Ax and BoTorch: New AI tools for adaptive experimentation](https://ai.facebook.com/blog/open-sourcing-ax-and-botorch-new-ai-tools-for-adaptive-experimentation/)\n",
    "* [A Visual Exploration of Gaussian Processes](https://distill.pub/2019/visual-exploration-gaussian-processes/)\n",
    "* [Gaussian Processes for Machine Learning](http://www.gaussianprocess.org/gpml/chapters/RW.pdf)\n",
    "* [Automatic Model Construction with Gaussian Processes](https://www.cs.toronto.edu/~duvenaud/thesis.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ¬°Gracias! ü§ì\n",
    "\n",
    "### Ra√∫l Peralta Lozada\n",
    "* Blog: https://raulpl.github.io/\n",
    "* Twitter: @tnak25\n",
    "* email: raulpl25@gmail.com\n",
    "* OPI Analytics"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
